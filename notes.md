## 6. 卷积神经网络

### 6.5 汇聚层

汇聚层的作用：降低卷积层对位置的敏感性，同时降低对空间降采样表示的敏感性

### 6.6 LeNet

1989年提出，LeNet被广泛用于自动取款机（ATM）机中，帮助识别处理支票的数字。

## 7. 现代卷积神经网络

### 7.1 AlexNet

经典机器学习流水线：

1. 获取一个有趣的数据集。在早期，收集这些数据集需要昂贵的传感器（在当时最先进的图像也就100万像素）。
2. 根据光学、几何学、其他知识以及偶然的发现，手工对特征数据集进行预处理。
3. 通过标准的特征提取算法，如SIFT（尺度不变特征变换） [[Lowe, 2004\]](https://zh.d2l.ai/chapter_references/zreferences.html#lowe-2004)和SURF（加速鲁棒特征） [[Bay et al., 2006\]](https://zh.d2l.ai/chapter_references/zreferences.html#bay-tuytelaars-van-gool-2006)或其他手动调整的流水线来输入数据。
4. 将提取的特征送入最喜欢的分类器中（例如线性模型或其它核方法），以训练分类器。

深度神经网络突破的两个关键因素：

1. 数据。ImageNet，李飞飞，亚马逊众包
2. 硬件。GPU内核比CPU更简单更节能，GPU带宽是CPU10倍。[cuda-convnet](https://code.google.com/archive/p/cuda-convnet/)

### 7.2 VGG

VGG块的基本组成元素：使用了带有3×3卷积核、填充为1（保持高度和宽度）的卷积层，和带有2×2汇聚窗口、步幅为2（每个块后的分辨率减半）的最大汇聚层。

### 7.3 NiN

NiN块使用两个1x1的卷积层代替全连接层。减少了模型参数，可能会增加训练时间。

### 7.4 GoogLeNet

有时使用不同大小的卷积核组合是有利的。在Inception块中，通常调整的超参数是每层输出通道数。GoogLeNet一共使用9个Inception块和全局平均汇聚层的堆叠来生成其估计值。Inception块之间的最大汇聚层可降低维度。

- Inception块相当于一个有4条路径的子网络。它通过不同窗口形状的卷积层和最大汇聚层来并行抽取信息，并使用1×11×1卷积层减少每像素级别上的通道维数从而降低模型复杂度。
- GoogLeNet将多个设计精细的Inception块与其他层（卷积层、全连接层）串联起来。其中Inception块的通道数分配之比是在ImageNet数据集上通过大量的实验得来的。
- GoogLeNet和它的后继者们一度是ImageNet上最有效的模型之一：它以较低的计算复杂度提供了类似的测试精度。